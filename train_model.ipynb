{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heart Disease Risk Prediction - Model Training\n",
        "\n",
        "This notebook implements a machine learning pipeline to predict heart disease risk using the BRFSS (Behavioral Risk Factor Surveillance System) dataset.\n",
        "\n",
        "## Approach\n",
        "1. Load and preprocess the data\n",
        "2. Feature selection based on correlation with target variable\n",
        "3. Train a Linear SVM classifier with class balancing\n",
        "4. Optimize decision threshold for better F1-score\n",
        "5. Generate predictions on test set\n",
        "\n",
        "## Model Performance\n",
        "- **Algorithm**: Linear Support Vector Machine (LinearSVC)\n",
        "- **Validation F1-score**: ~0.366 (with threshold optimization)\n",
        "- **Key features**: 23 features selected based on |correlation| > 0.1 with target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import f1_score, classification_report, precision_recall_curve\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training and test datasets\n",
        "train_df = pd.read_csv('data/train.csv')\n",
        "test_df = pd.read_csv('data/test.csv')\n",
        "\n",
        "print(f\"Train dataset shape: {train_df.shape}\")\n",
        "print(f\"Test dataset shape: {test_df.shape}\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(train_df['TARGET'].value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "Remove columns that are 100% NaN (no information)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove columns with 100% missing values\n",
        "cols_all_nan_train = train_df.columns[train_df.isna().mean() == 1.0]\n",
        "train_df = train_df.drop(columns=cols_all_nan_train)\n",
        "\n",
        "cols_all_nan_test = test_df.columns[test_df.isna().mean() == 1.0]\n",
        "test_df = test_df.drop(columns=cols_all_nan_test)\n",
        "\n",
        "print(f\"Removed {len(cols_all_nan_train)} columns with 100% NaN from train\")\n",
        "print(f\"Removed {len(cols_all_nan_test)} columns with 100% NaN from test\")\n",
        "print(f\"\\nCleaned train shape: {train_df.shape}\")\n",
        "print(f\"Cleaned test shape: {test_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Selection\n",
        "\n",
        "Select features with |correlation| > 0.1 with the target variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify target column\n",
        "target_col = train_df.columns[-1]\n",
        "print(f\"Target variable: {target_col}\")\n",
        "\n",
        "# Convert to numeric for correlation computation\n",
        "train_num = train_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Compute correlation with target\n",
        "corr_with_target = train_num.corr(numeric_only=True)[target_col].drop(labels=[target_col], errors='ignore')\n",
        "\n",
        "# Select features with |correlation| > 0.1\n",
        "selected_features = corr_with_target[abs(corr_with_target) > 0.1].index.tolist()\n",
        "\n",
        "print(f\"\\nNumber of features selected: {len(selected_features)}\")\n",
        "print(f\"\\nTop 10 correlated features:\")\n",
        "print(corr_with_target.abs().sort_values(ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features and target\n",
        "X = train_df[selected_features].copy()\n",
        "y = train_df[target_col]\n",
        "\n",
        "# Encode categorical variables\n",
        "encoder_dict = {}\n",
        "for col in X.select_dtypes(include=['object', 'category']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    encoder_dict[col] = le\n",
        "\n",
        "print(f\"Encoded {len(encoder_dict)} categorical columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train-Validation Split\n",
        "\n",
        "Use stratified split to maintain class distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data with stratification (important for imbalanced dataset)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.3, \n",
        "    stratify=y, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Training\n",
        "\n",
        "Pipeline with:\n",
        "- SimpleImputer: handle missing values\n",
        "- StandardScaler: normalize features\n",
        "- LinearSVC: SVM classifier with class balancing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pipeline\n",
        "pipe = make_pipeline(\n",
        "    SimpleImputer(strategy='most_frequent'),\n",
        "    StandardScaler(),\n",
        "    LinearSVC(C=1, class_weight='balanced', random_state=42, max_iter=2000)\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"Training model...\")\n",
        "pipe.fit(X_train, y_train)\n",
        "print(\"✓ Model trained successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Evaluation & Threshold Optimization\n",
        "\n",
        "Since the dataset is imbalanced, we optimize the decision threshold to maximize F1-score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard prediction (threshold = 0)\n",
        "y_pred_val = pipe.predict(X_val)\n",
        "f1_standard = f1_score(y_val, y_pred_val)\n",
        "print(f\"F1-score with default threshold: {f1_standard:.4f}\")\n",
        "print(\"\\nClassification Report (default threshold):\")\n",
        "print(classification_report(y_val, y_pred_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find optimal threshold using decision function\n",
        "scores_val = pipe.decision_function(X_val)\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, scores_val)\n",
        "\n",
        "# Compute F1-score for each threshold\n",
        "f1_scores = 2 * precision * recall / (precision + recall + 1e-12)\n",
        "\n",
        "# Find best threshold (excluding last point which has no threshold)\n",
        "best_idx = f1_scores[:-1].argmax()\n",
        "best_thresh = thresholds[best_idx]\n",
        "\n",
        "print(f\"\\nOptimal threshold: {best_thresh:.4f}\")\n",
        "print(f\"Best F1-score: {f1_scores[best_idx]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict with optimized threshold\n",
        "y_pred_val_optimized = (scores_val >= best_thresh).astype(int)\n",
        "f1_optimized = f1_score(y_val, y_pred_val_optimized)\n",
        "\n",
        "print(f\"F1-score with optimized threshold: {f1_optimized:.4f}\")\n",
        "print(\"\\nClassification Report (optimized threshold):\")\n",
        "print(classification_report(y_val, y_pred_val_optimized))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Predictions on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare test data with same features\n",
        "X_test = test_df.reindex(columns=selected_features, fill_value=np.nan).copy()\n",
        "\n",
        "# Apply same encoding to categorical features\n",
        "for col, le in encoder_dict.items():\n",
        "    if col in X_test.columns:\n",
        "        X_test[col] = X_test[col].astype(str)\n",
        "        # Map unknown values to the first class\n",
        "        X_test[col] = X_test[col].map(lambda x: x if x in le.classes_ else None)\n",
        "        X_test[col] = le.transform(X_test[col].fillna(le.classes_[0]))\n",
        "\n",
        "print(\"Test data prepared\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get decision function scores\n",
        "scores_test = pipe.decision_function(X_test)\n",
        "\n",
        "# Apply optimized threshold\n",
        "y_test_pred = (scores_test >= best_thresh).astype(int)\n",
        "\n",
        "print(f\"Generated {len(y_test_pred)} predictions\")\n",
        "print(f\"\\nPrediction distribution:\")\n",
        "print(pd.Series(y_test_pred).value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get ID column from test set\n",
        "id_col = 'ID' if 'ID' in test_df.columns else test_df.columns[0]\n",
        "\n",
        "# Create submission dataframe\n",
        "submission = pd.DataFrame({\n",
        "    id_col: test_df[id_col],\n",
        "    'pred': y_test_pred\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('predictions_svm_optimized.csv', index=False)\n",
        "\n",
        "print(\"✓ Predictions exported to 'predictions_svm_optimized.csv'\")\n",
        "print(\"\\nFirst 10 predictions:\")\n",
        "print(submission.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates:\n",
        "- Feature selection based on correlation analysis\n",
        "- Handling imbalanced datasets with class weighting\n",
        "- Threshold optimization to improve F1-score\n",
        "- Complete pipeline from data loading to predictions\n",
        "\n",
        "### Key Takeaways:\n",
        "- The dataset is highly imbalanced (~9% positive class)\n",
        "- Feature selection reduced dimensionality from 324 to 23 features\n",
        "- Threshold optimization significantly improved F1-score\n",
        "- StandardScaler is crucial for SVM performance\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
